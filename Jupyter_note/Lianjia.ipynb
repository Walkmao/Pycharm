{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas\n",
    "import re\n",
    "import fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://su.lianjia.com/ershoufang/pg1\n",
      "https://su.lianjia.com/ershoufang/pg2\n",
      "https://su.lianjia.com/ershoufang/pg3\n",
      "https://su.lianjia.com/ershoufang/pg4\n",
      "https://su.lianjia.com/ershoufang/pg5\n",
      "https://su.lianjia.com/ershoufang/pg6\n",
      "https://su.lianjia.com/ershoufang/pg7\n",
      "https://su.lianjia.com/ershoufang/pg8\n",
      "https://su.lianjia.com/ershoufang/pg9\n",
      "https://su.lianjia.com/ershoufang/pg10\n",
      "['https://su.lianjia.com/ershoufang/pg1', 'https://su.lianjia.com/ershoufang/pg2', 'https://su.lianjia.com/ershoufang/pg3', 'https://su.lianjia.com/ershoufang/pg4', 'https://su.lianjia.com/ershoufang/pg5', 'https://su.lianjia.com/ershoufang/pg6', 'https://su.lianjia.com/ershoufang/pg7', 'https://su.lianjia.com/ershoufang/pg8', 'https://su.lianjia.com/ershoufang/pg9', 'https://su.lianjia.com/ershoufang/pg10']\n"
     ]
    }
   ],
   "source": [
    "rurl = 'https://su.lianjia.com/ershoufang/pg%s'\n",
    "number = 10\n",
    "urlist = []\n",
    "for i in range(1,number+1):\n",
    "    urli = rurl % i\n",
    "    print (urli)\n",
    "    urlist.append(urli)\n",
    "print (urlist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.headers = {\"User-Agent\": UserAgent().random}\n",
    "    self.datas = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成所有页面的网址url\n",
    "def generate_ulr(url,n):\n",
    "    urlist = []\n",
    "    for i in range(1,n+1):\n",
    "        urli = url % i\n",
    "        urlist.append(urli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://su.lianjia.com/ershoufang/107101693531.html\n"
     ]
    }
   ],
   "source": [
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}\n",
    "res = requests.get(urlist[0],headers = header)\n",
    "soup = bs(res.text,'lxml')\n",
    "urls = soup.find_all('a',class_ =\"noresultRecommend img LOGCLICKDATA\")\n",
    "urlslist = []\n",
    "for i in urls:\n",
    "    urlslist.append(i[\"href\"])\n",
    "    #print (i[\"href\"])\n",
    "print(urlslist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取每一页的详情页url\n",
    "def get_url(generate_url):\n",
    "    urlslist = []\n",
    "    res = requests.get(generate_url,headers = self.headers)\n",
    "    soup = bs(res.text,'lxml')\n",
    "    urls = soup.find_all('a',class_ =\"noresultRecommend img LOGCLICKDATA\")\n",
    "    for i in urls:\n",
    "        urlslist.append(i[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}\n",
    "resp = requests.get(urlist[0],headers = headers)\n",
    "resp.text\n",
    "resp.status_code\n",
    "soup = bs(resp.text,'lxml')\n",
    "page = soup.find('div',class_=\"page-box house-lst-page-box\")\n",
    "print(page[\"page-data\"])\n",
    "page_n = eval(page[\"page-data\"])\n",
    "print(page_n)\n",
    "print(page_n[\"totalPage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取网页的最大页码totalpage\n",
    "def total_pages(self,url):\n",
    "    res = request.get(url,headers = self.headers)\n",
    "    if res.status_code == 200:\n",
    "        soup = bs(res.text,'lxml')\n",
    "        page = soup.find('div',class_=\"page-box house-lst-page-box\")\n",
    "        total_page = eval(page[\"page-date\"])['totalPage']\n",
    "    else:\n",
    "        print(\"fail stautus:{}\".format(res.status_code))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "url1 = urlslist[0]\n",
    "r = requests.get(url1)\n",
    "soup = bs(r.text,'lxml')\n",
    "dic = {}\n",
    "dic['title'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url(get_url):\n",
    "    r = requests.get(get_url)\n",
    "    soup = bs(r.text, 'lxml')\n",
    "    ul = soup.find('ul', class_=\"list_item clrfix\")      #获取包含当前页面信息list的块，并返回字符串类型\n",
    "    li = ul.find_all('li') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
